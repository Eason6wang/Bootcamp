{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_answer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kKlTG6ye_16V",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AVHNfJI_5GL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d95cfc84-804d-4718-e998-035f3726c7cb"
      },
      "source": [
        "# fix random seed so we can get the same result\n",
        "np.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "\n",
        "# only want to retrieve the most frequent 5000 words\n",
        "num_words = 5000 \n",
        "np_load_old = np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "np.load = np_load_old\n",
        "\n",
        "# truncate and pad input sequences to the same length\n",
        "\n",
        "# The max input size is 500\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pAC92NWISaXO",
        "outputId": "95540a3b-fd2e-4abe-e664-494e48cffe82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "print('Train data shape:', X_train.shape)\n",
        "print('Test data shape:', X_test.shape)\n",
        "\n",
        "# you can print out the first example to see what it looks like\n",
        "\n",
        "print('The first training example:')\n",
        "print(X_train[0])\n",
        "print('The first training label:')\n",
        "print(y_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (25000, 500)\n",
            "Test data shape: (25000, 500)\n",
            "The first training example:\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16    2   19  178   32]\n",
            "The first training label:\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vwNOJLmSkB-",
        "colab": {}
      },
      "source": [
        "'''\n",
        "P1\n",
        "We want to randomly\n",
        "\n",
        "- pick 5000 instances from training data\n",
        "- pick 1000 instances from testing data\n",
        "\n",
        "'''\n",
        "train_indexes = np.random.choice(len(X_train), 5000, replace=False)\n",
        "test_indexes = np.random.choice(len(X_test), 1000, replace=False)\n",
        "X_train = X_train[train_indexes]\n",
        "y_train = y_train[train_indexes]\n",
        "X_test = X_test[test_indexes]\n",
        "y_test = y_test[test_indexes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IZ_yEeHfCAQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "2a45fe48-0ff4-4d89-891d-cc79772880fb"
      },
      "source": [
        "'''\n",
        "P2\n",
        "\n",
        "In this section, we want to construct a LSTM model\n",
        "\n",
        "Outline:\n",
        "sequence of words -> word embeddings(input) -> dropout 20% of the input ->\n",
        "LSTM of 100 units -> dropout 20% of the output -> sigmoid activation -> 0 or 1\n",
        "\n",
        "complete the model graph\n",
        "- add input dropout\n",
        "- add lstm layer\n",
        "- add output dropout\n",
        "- use sigmoid activation\n",
        "- use binary_crossentropy loss function\n",
        "\n",
        "'''\n",
        "\n",
        "# create the model\n",
        "vec_len = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, vec_len, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0706 19:11:40.702961 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0706 19:11:40.737390 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0706 19:11:40.744193 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0706 19:11:40.858706 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0706 19:11:40.874373 139721036928896 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0706 19:11:41.148010 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0706 19:11:41.169325 139721036928896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0706 19:11:41.175715 139721036928896 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WcpsRJ0HCSvh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7c29fb07-cbbf-4c26-9009-7a08e545bcf5"
      },
      "source": [
        "'''\n",
        "P3\n",
        "- train 3 epochs with batch size of 64\n",
        "- do evaluation on testing data\n",
        "\n",
        "'''\n",
        "\n",
        "# training and evaluation\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "5000/5000 [==============================] - 69s 14ms/step - loss: 0.6726 - acc: 0.5880\n",
            "Epoch 2/3\n",
            "5000/5000 [==============================] - 64s 13ms/step - loss: 0.4545 - acc: 0.8006\n",
            "Epoch 3/3\n",
            "5000/5000 [==============================] - 64s 13ms/step - loss: 0.3331 - acc: 0.8646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6rPYlautSw5f",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "de9rI5vYCa-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "adefd913-138b-438b-fa1e-dcd2d8b2d118"
      },
      "source": [
        "# create the model\n",
        "vec_len = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, vec_len, input_length=max_review_length))\n",
        "model.add(SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 100)               13300     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 173,401\n",
            "Trainable params: 173,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "5000/5000 [==============================] - 23s 5ms/step - loss: 0.7154 - acc: 0.5004\n",
            "Epoch 2/3\n",
            "5000/5000 [==============================] - 22s 4ms/step - loss: 0.6996 - acc: 0.5208\n",
            "Epoch 3/3\n",
            "5000/5000 [==============================] - 22s 4ms/step - loss: 0.7032 - acc: 0.5176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQeVgDfQtkOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4251a3d0-fbb8-49ec-ead6-c17bf5d08bfc"
      },
      "source": [
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 47.30%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hYv6DbiuKwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}